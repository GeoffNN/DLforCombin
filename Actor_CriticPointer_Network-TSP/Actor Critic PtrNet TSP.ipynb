{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tsp_env\n",
    "from pointer_net_tsp import *\n",
    "from critic_network_tsp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128; n_coords = 2; n_cities = 5; hidden_size = 128; \n",
    "embedding_size = hidden_size\n",
    "# Define the network and placeholders\n",
    "cities_coords_ph = tf.placeholder(tf.float32, [batch_size, n_cities, n_coords])\n",
    "played_actions_ph = tf.placeholder(tf.int32, [batch_size, n_cities])\n",
    "rewards_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "adv_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "\n",
    "# Actor network definition\n",
    "loss, decoder_outputs, total_loss = pointer_network(cities_coords_ph,\n",
    "                                                    played_actions_ph,\n",
    "                                                    hidden_size=hidden_size,\n",
    "                                                    embedding_size=embedding_size,\n",
    "                                                    max_time_steps=n_cities,\n",
    "                                                    input_size=n_coords,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    initialization_stddev=0.1)\n",
    "\n",
    "# Critic network definition\n",
    "bsln_value = critic_network(cities_coords_ph,\n",
    "                            hidden_size=hidden_size,\n",
    "                            embedding_size=embedding_size,\n",
    "                            max_time_steps=n_cities,\n",
    "                            input_size=n_coords,\n",
    "                            batch_size=batch_size,\n",
    "                            initialization_stddev=0.1,\n",
    "                            n_processing_steps=n_cities,\n",
    "                            d=embedding_size)\n",
    "\n",
    "# Cross entropy (loss here) is the negative log likelihood of taken actions\n",
    "# Rewards is negative tour length\n",
    "# We want to maximize E[logprob(\\tau) * reward(\\tau)]\n",
    "# I.e. minimize E[logprob(tau) * tour_length(\\tau)]\n",
    "# I.e. minimize -E[cross entropy(tau) * tour_length(\\tau)]\n",
    "# I.e. minimize E[cross_entropy(\\tau) * reward(\\tau)] = E[loss(\\tau) * reward(\\tau)]\n",
    "pg_loss = tf.reduce_sum(loss * adv_ph)\n",
    "optimizer = tf.train.AdamOptimizer(2e-3)\n",
    "update_op = optimizer.minimize(pg_loss)\n",
    "\n",
    "# Baseline loss and training op\n",
    "bsln_loss = tf.losses.mean_squared_error(labels=rewards_ph,\n",
    "                                         predictions=bsln_value)\n",
    "bsln_train_op = tf.train.AdamOptimizer(2e-3).minimize(bsln_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward:  -2.63844886844\n",
      "Baseline MSE:  0.52204\n",
      "Mean reward:  -2.38110814915\n",
      "Baseline MSE:  0.0704803\n",
      "Mean reward:  -2.39483235433\n",
      "Baseline MSE:  0.0675375\n",
      "Mean reward:  -2.19432494969\n",
      "Baseline MSE:  0.0106988\n"
     ]
    }
   ],
   "source": [
    "n_baseline_gradient_steps = 10\n",
    "#################################\n",
    "#        POLICY GRADIENT        # \n",
    "#################################\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "# Iterate batch collection and gradient steps\n",
    "for it in range(int(1e5)):\n",
    "    # Collect batch\n",
    "    envs = []\n",
    "    inputs_list = []\n",
    "    # Generate and initialize a batch of environments\n",
    "    for i in range(batch_size):\n",
    "        envs.append(tsp_env.TSP_env(n_cities, use_alternative_state=True))\n",
    "        envs[-1].reset()\n",
    "        inputs_list.append(envs[-1].nodes)\n",
    "\n",
    "    inputs_batch = np.array(inputs_list)\n",
    "    # Use the PointerNet on this test batch and get its predictions\n",
    "    predicted_outputs = np.array(sess.run(decoder_outputs, \n",
    "                                          feed_dict={cities_coords_ph: inputs_batch})).T\n",
    "    # Compute the rewards corresponding to the predicted tours\n",
    "    rewards = []\n",
    "    for i in range(batch_size):\n",
    "        for action in predicted_outputs[i]:\n",
    "            envs[i].step(action)\n",
    "        rewards.append(envs[i].accumulated_reward())\n",
    "        \n",
    "    # Carry out baseline training steps\n",
    "    for bsln_step in range(n_baseline_gradient_steps):\n",
    "        bsln_loss_val, _ = sess.run([bsln_loss, bsln_train_op], \n",
    "                                    feed_dict={cities_coords_ph: inputs_batch,\n",
    "                                               rewards_ph: rewards})\n",
    "    \n",
    "    # Compute baseline value\n",
    "    bsln = sess.run(bsln_value, feed_dict={cities_coords_ph: inputs_batch})\n",
    "    \n",
    "    # Compute normalized advantages\n",
    "    adv = np.array(rewards) - bsln\n",
    "    normd_adv = (adv - np.mean(adv)) / np.std(adv)\n",
    "    # Print average reward\n",
    "    if it % 100 == 0:\n",
    "        print('Mean reward: ', np.mean(rewards))\n",
    "        print('Baseline MSE: ', bsln_loss_val)\n",
    "    \n",
    "    # Take a gradient step\n",
    "    sess.run(update_op, feed_dict={cities_coords_ph: inputs_batch,\n",
    "                                   played_actions_ph: predicted_outputs,\n",
    "                                   adv_ph: normd_adv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: supervised training\n",
    "def generate_batch(n_cities, batch_size):\n",
    "    inputs_list = []; labels_list = []\n",
    "    env = tsp_env.TSP_env(n_cities, use_alternative_state=True)\n",
    "    for i in range(batch_size):\n",
    "        env.reset()\n",
    "        s = env.reset()\n",
    "        coords = s.reshape([4, n_cities])[:2, ].T\n",
    "        inputs_list.append(coords)\n",
    "        labels_list.append(env.optimal_solution()[1])\n",
    "    return np.array(inputs_list), np.array(labels_list)\n",
    "\n",
    "supervised_update_op = tf.train.AdamOptimizer(2e-3).minimize(total_loss)\n",
    "# Define session, initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "# Training loop\n",
    "mean_approx_ratios = []\n",
    "loss_vals = []\n",
    "for i in range(1000):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    inputs_batch, labels_batch = generate_batch(n_cities, batch_size)\n",
    "    loss_val, _ = sess.run([total_loss, supervised_update_op], \n",
    "                         feed_dict={cities_coords_ph: inputs_batch,\n",
    "                                    played_actions_ph: labels_batch})\n",
    "    loss_vals.append(loss_val)\n",
    "    # Test accuracy\n",
    "    if i % 100 == 0:\n",
    "        envs = []\n",
    "        inputs_list = []\n",
    "        optimal_rewards = []\n",
    "        optimal_tours = []\n",
    "        # Generate and initialize a batch of environments\n",
    "        for i in range(batch_size):\n",
    "            envs.append(tsp_env.TSP_env(n_cities, use_alternative_state=True))\n",
    "            envs[-1].reset()\n",
    "            inputs_list.append(envs[-1].nodes)\n",
    "            optimal_solution = envs[-1].optimal_solution()\n",
    "            optimal_rewards.append(optimal_solution[0])\n",
    "            optimal_tours.append(optimal_solution[1])\n",
    "        inputs_batch = np.array(inputs_list)\n",
    "        # Use the PointerNet on this test batch and get its predictions\n",
    "        predicted_outputs = np.array(sess.run(decoder_outputs, \n",
    "                                              feed_dict={cities_coords_ph: inputs_batch})).T\n",
    "        # Compute the rewards corresponding to the predicted tours\n",
    "        rewards = []\n",
    "        for i in range(batch_size):\n",
    "            for action in predicted_outputs[i]:\n",
    "                envs[i].step(action)\n",
    "            rewards.append(envs[i].accumulated_reward())\n",
    "        # Get the approximation ratio of the predictions\n",
    "        approximation_ratios = np.array(rewards) / np.array(optimal_rewards)\n",
    "        mean_approx_ratios.append(np.mean(approximation_ratios))\n",
    "        print(mean_approx_ratios[-1])\n",
    "        print(loss_vals[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
