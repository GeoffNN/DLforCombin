{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tsp_env\n",
    "# from pointer_net_tsp import *\n",
    "from pointer_net_tsp_cnn import *\n",
    "from critic_network_tsp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128; n_coords = 2; n_cities = 5; hidden_size = 128; \n",
    "embedding_size = hidden_size\n",
    "# Define the network and placeholders\n",
    "cities_coords_ph = tf.placeholder(tf.float32, [batch_size, n_cities, n_coords])\n",
    "played_actions_ph = tf.placeholder(tf.int32, [batch_size, n_cities])\n",
    "rewards_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "adv_ph = tf.placeholder(tf.float32, [batch_size])\n",
    "\n",
    "# Actor network definition\n",
    "loss, decoder_outputs, total_loss = pointer_network(cities_coords_ph,\n",
    "                                                    played_actions_ph,\n",
    "                                                    hidden_size=hidden_size,\n",
    "                                                    embedding_size=embedding_size,\n",
    "                                                    max_time_steps=n_cities,\n",
    "                                                    input_size=n_coords,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    initialization_stddev=0.1)\n",
    "\n",
    "# Critic network definition\n",
    "bsln_value = critic_network(cities_coords_ph,\n",
    "                            hidden_size=hidden_size,\n",
    "                            embedding_size=embedding_size,\n",
    "                            max_time_steps=n_cities,\n",
    "                            input_size=n_coords,\n",
    "                            batch_size=batch_size,\n",
    "                            initialization_stddev=0.1,\n",
    "                            n_processing_steps=n_cities,\n",
    "                            d=embedding_size)\n",
    "\n",
    "# Cross entropy (loss here) is the negative log likelihood of taken actions\n",
    "# Rewards is negative tour length\n",
    "# We want to maximize E[logprob(\\tau) * reward(\\tau)]\n",
    "# I.e. minimize E[logprob(tau) * tour_length(\\tau)]\n",
    "# I.e. minimize -E[cross entropy(tau) * tour_length(\\tau)]\n",
    "# I.e. minimize E[cross_entropy(\\tau) * reward(\\tau)] = E[loss(\\tau) * reward(\\tau)]\n",
    "pg_loss = tf.reduce_sum(loss * adv_ph)\n",
    "optimizer = tf.train.AdamOptimizer(2e-3)\n",
    "update_op = optimizer.minimize(pg_loss)\n",
    "\n",
    "# Baseline loss and training op\n",
    "bsln_loss = tf.losses.mean_squared_error(labels=rewards_ph,\n",
    "                                         predictions=bsln_value)\n",
    "bsln_train_op = tf.train.AdamOptimizer(2e-3).minimize(bsln_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward:  -2.63118504469\n",
      "Baseline MSE:  0.447038\n",
      "Mean approx ratio:  1.22205727752\n",
      "Mean reward:  -2.44070418581\n",
      "Baseline MSE:  0.0705056\n",
      "Mean approx ratio:  1.14067852106\n",
      "Mean reward:  -2.44796928395\n",
      "Baseline MSE:  0.0351494\n",
      "Mean approx ratio:  1.1275777566\n",
      "Mean reward:  -2.39447404604\n",
      "Baseline MSE:  0.039054\n",
      "Mean approx ratio:  1.10565639148\n",
      "Mean reward:  -2.38047431486\n",
      "Baseline MSE:  0.0356203\n",
      "Mean approx ratio:  1.10071805783\n",
      "Mean reward:  -2.44285110153\n",
      "Baseline MSE:  0.0485424\n",
      "Mean approx ratio:  1.10248474239\n",
      "Mean reward:  -2.37843733664\n",
      "Baseline MSE:  0.0475589\n",
      "Mean approx ratio:  1.1256594649\n",
      "Mean reward:  -2.15641648446\n",
      "Baseline MSE:  0.0166612\n",
      "Mean approx ratio:  1.03313285501\n",
      "Mean reward:  -2.21132375081\n",
      "Baseline MSE:  0.00870712\n",
      "Mean approx ratio:  1.0213839141\n",
      "Mean reward:  -2.08940205379\n",
      "Baseline MSE:  0.00545849\n",
      "Mean approx ratio:  1.01568887501\n",
      "Mean reward:  -2.18464757904\n",
      "Baseline MSE:  0.00753141\n",
      "Mean approx ratio:  1.01047520539\n",
      "Mean reward:  -2.20159193477\n",
      "Baseline MSE:  0.00674402\n",
      "Mean approx ratio:  1.00827292281\n",
      "Mean reward:  -2.11716176936\n",
      "Baseline MSE:  0.00497725\n",
      "Mean approx ratio:  1.01944068262\n",
      "Mean reward:  -2.15178358043\n",
      "Baseline MSE:  0.00422245\n",
      "Mean approx ratio:  1.00975830271\n",
      "Mean reward:  -2.22372033825\n",
      "Baseline MSE:  0.0191042\n",
      "Mean approx ratio:  1.01792129147\n",
      "Mean reward:  -2.14117404701\n",
      "Baseline MSE:  0.00366\n",
      "Mean approx ratio:  1.01152152662\n",
      "Mean reward:  -2.07490670263\n",
      "Baseline MSE:  0.00448898\n",
      "Mean approx ratio:  1.00847434408\n",
      "Mean reward:  -2.08280027169\n",
      "Baseline MSE:  0.00430562\n",
      "Mean approx ratio:  1.00755521992\n",
      "Mean reward:  -2.08354048838\n",
      "Baseline MSE:  0.00383616\n",
      "Mean approx ratio:  1.01365671818\n",
      "Mean reward:  -2.1549724693\n",
      "Baseline MSE:  0.00418452\n",
      "Mean approx ratio:  1.01485243442\n",
      "Mean reward:  -2.19587177747\n",
      "Baseline MSE:  0.00339364\n",
      "Mean approx ratio:  1.00863662404\n",
      "Mean reward:  -2.17683635502\n",
      "Baseline MSE:  0.00720955\n",
      "Mean approx ratio:  1.01921765416\n",
      "Mean reward:  -2.12014056011\n",
      "Baseline MSE:  0.00583811\n",
      "Mean approx ratio:  1.01036643067\n",
      "Mean reward:  -2.21459243349\n",
      "Baseline MSE:  0.00232937\n",
      "Mean approx ratio:  1.00851035355\n",
      "Mean reward:  -2.09121947274\n",
      "Baseline MSE:  0.00832493\n",
      "Mean approx ratio:  1.01815828223\n",
      "Mean reward:  -2.16744938969\n",
      "Baseline MSE:  0.0058789\n",
      "Mean approx ratio:  1.00848628825\n",
      "Mean reward:  -2.07484787581\n",
      "Baseline MSE:  0.00279366\n",
      "Mean approx ratio:  1.01041634543\n",
      "Mean reward:  -2.13665885697\n",
      "Baseline MSE:  0.00524961\n",
      "Mean approx ratio:  1.01700793753\n",
      "Mean reward:  -2.19324104623\n",
      "Baseline MSE:  0.00694928\n",
      "Mean approx ratio:  1.01579194622\n",
      "Mean reward:  -2.09464911601\n",
      "Baseline MSE:  0.00370494\n",
      "Mean approx ratio:  1.01476657682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9cb26f94e86c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         bsln_loss_val, _ = sess.run([bsln_loss, bsln_train_op], \n\u001b[1;32m     32\u001b[0m                                     feed_dict={cities_coords_ph: inputs_batch,\n\u001b[0;32m---> 33\u001b[0;31m                                                rewards_ph: rewards})\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Compute baseline value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_baseline_gradient_steps = 10\n",
    "mean_approx_ratios = []\n",
    "#################################\n",
    "#        POLICY GRADIENT        # \n",
    "#################################\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "# Iterate batch collection and gradient steps\n",
    "for it in range(int(100 * 20)):\n",
    "    # Collect batch\n",
    "    envs = []\n",
    "    inputs_list = []\n",
    "    # Generate and initialize a batch of environments\n",
    "    for i in range(batch_size):\n",
    "        envs.append(tsp_env.TSP_env(n_cities, use_alternative_state=True))\n",
    "        envs[-1].reset()\n",
    "        inputs_list.append(envs[-1].nodes)\n",
    "\n",
    "    inputs_batch = np.array(inputs_list)\n",
    "    # Use the PointerNet on this test batch and get its predictions\n",
    "    predicted_outputs = np.array(sess.run(decoder_outputs, \n",
    "                                          feed_dict={cities_coords_ph: inputs_batch})).T\n",
    "    # Compute the rewards corresponding to the predicted tours\n",
    "    rewards = []\n",
    "    for i in range(batch_size):\n",
    "        for action in predicted_outputs[i]:\n",
    "            envs[i].step(action)\n",
    "        rewards.append(envs[i].accumulated_reward())\n",
    "        \n",
    "    # Carry out baseline training steps\n",
    "    for bsln_step in range(n_baseline_gradient_steps):\n",
    "        bsln_loss_val, _ = sess.run([bsln_loss, bsln_train_op], \n",
    "                                    feed_dict={cities_coords_ph: inputs_batch,\n",
    "                                               rewards_ph: rewards})\n",
    "    \n",
    "    # Compute baseline value\n",
    "    bsln = sess.run(bsln_value, feed_dict={cities_coords_ph: inputs_batch})\n",
    "    \n",
    "    # Compute normalized advantages\n",
    "    adv = np.array(rewards) - bsln\n",
    "    normd_adv = (adv - np.mean(adv)) / np.std(adv)\n",
    "    \n",
    "    \n",
    "    # Print average reward\n",
    "    if it % 100 == 0:\n",
    "        print('Mean reward: ', np.mean(rewards))\n",
    "        print('Baseline MSE: ', bsln_loss_val)\n",
    "        # Get approximation ratio\n",
    "        test_envs = []\n",
    "        test_inputs_list = []\n",
    "        test_optimal_rewards = []\n",
    "        test_optimal_tours = []\n",
    "        # Generate and initialize a batch of environments\n",
    "        for i in range(batch_size):\n",
    "            test_envs.append(tsp_env.TSP_env(n_cities, use_alternative_state=True))\n",
    "            test_envs[-1].reset()\n",
    "            test_inputs_list.append(test_envs[-1].nodes)\n",
    "            test_optimal_solution = test_envs[-1].optimal_solution()\n",
    "            test_optimal_rewards.append(test_optimal_solution[0])\n",
    "            test_optimal_tours.append(test_optimal_solution[1])\n",
    "        test_inputs_batch = np.array(test_inputs_list)\n",
    "        # Use the PointerNet on this test batch and get its predictions\n",
    "        test_predicted_outputs = np.array(sess.run(decoder_outputs,\n",
    "                                                   feed_dict={cities_coords_ph: test_inputs_batch})).T\n",
    "        # Compute the rewards corresponding to the predicted tours\n",
    "        test_rewards = []\n",
    "        for k in range(batch_size):\n",
    "            for test_action in test_predicted_outputs[k]:\n",
    "                test_envs[k].step(test_action)\n",
    "            test_rewards.append(test_envs[k].accumulated_reward())\n",
    "        # Get the approximation ratio of the predictions\n",
    "        approximation_ratios = np.array(test_rewards) / np.array(test_optimal_rewards)\n",
    "        mean_approx_ratios.append(np.mean(approximation_ratios))\n",
    "        print('Mean approx ratio: ', mean_approx_ratios[-1])\n",
    "    \n",
    "    # Take a gradient step\n",
    "    sess.run(update_op, feed_dict={cities_coords_ph: inputs_batch,\n",
    "                                   played_actions_ph: predicted_outputs,\n",
    "                                   adv_ph: normd_adv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9c5e0ec780>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c9f06df98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(mean_approx_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_approx_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: supervised training\n",
    "def generate_batch(n_cities, batch_size):\n",
    "    inputs_list = []; labels_list = []\n",
    "    env = tsp_env.TSP_env(n_cities, use_alternative_state=True)\n",
    "    for i in range(batch_size):\n",
    "        env.reset()\n",
    "        s = env.reset()\n",
    "        coords = s.reshape([4, n_cities])[:2, ].T\n",
    "        inputs_list.append(coords)\n",
    "        labels_list.append(env.optimal_solution()[1])\n",
    "    return np.array(inputs_list), np.array(labels_list)\n",
    "\n",
    "supervised_update_op = tf.train.AdamOptimizer(2e-3).minimize(total_loss)\n",
    "# Define session, initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "# Training loop\n",
    "mean_approx_ratios = []\n",
    "loss_vals = []\n",
    "for i in range(1000):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    inputs_batch, labels_batch = generate_batch(n_cities, batch_size)\n",
    "    loss_val, _ = sess.run([total_loss, supervised_update_op], \n",
    "                         feed_dict={cities_coords_ph: inputs_batch,\n",
    "                                    played_actions_ph: labels_batch})\n",
    "    loss_vals.append(loss_val)\n",
    "    # Test accuracy\n",
    "    if i % 100 == 0:\n",
    "        envs = []\n",
    "        inputs_list = []\n",
    "        optimal_rewards = []\n",
    "        optimal_tours = []\n",
    "        # Generate and initialize a batch of environments\n",
    "        for i in range(batch_size):\n",
    "            envs.append(tsp_env.TSP_env(n_cities, use_alternative_state=True))\n",
    "            envs[-1].reset()\n",
    "            inputs_list.append(envs[-1].nodes)\n",
    "            optimal_solution = envs[-1].optimal_solution()\n",
    "            optimal_rewards.append(optimal_solution[0])\n",
    "            optimal_tours.append(optimal_solution[1])\n",
    "        inputs_batch = np.array(inputs_list)\n",
    "        # Use the PointerNet on this test batch and get its predictions\n",
    "        predicted_outputs = np.array(sess.run(decoder_outputs, \n",
    "                                              feed_dict={cities_coords_ph: inputs_batch})).T\n",
    "        # Compute the rewards corresponding to the predicted tours\n",
    "        rewards = []\n",
    "        for i in range(batch_size):\n",
    "            for action in predicted_outputs[i]:\n",
    "                envs[i].step(action)\n",
    "            rewards.append(envs[i].accumulated_reward())\n",
    "        # Get the approximation ratio of the predictions\n",
    "        approximation_ratios = np.array(rewards) / np.array(optimal_rewards)\n",
    "        mean_approx_ratios.append(np.mean(approximation_ratios))\n",
    "        print(mean_approx_ratios[-1])\n",
    "        print(loss_vals[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
